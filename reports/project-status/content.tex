\section{Introduction}
In this report, the status update of the CS574 NLP project is presented. An
overview is given of the progress made since the project proposal four weeks
previously. The main work done was to conduct additional literature research,
create a dataset, attempt to implement summarisation algorithms and to evaluate
them. Based on these results, the project will be rounded off in the remaining
two weeks.

\section{Dataset}
A dataset was collected from the internet of ten novels and a play, with at least three summaries each. The books are mainly fictional classics, due to their popularity on summary resources and often the expiration of their copyright. Most of the summaries are sourced from CliffsNotes, SparkNotes and GradeSaver. The books and summaries both vary quite vastly in length. All the documents were saved as or converted to text files, to facilitate consistency and ease of manipulation in Python.

\section{Algorithms}

Our hypothesis in summarising long texts is that differences between books are
important in the summary. That's why we've analysed the word frequencies in our
set of books. It becomes clear that often the most used, or at least the top 5
noun is referring to the main character. In books where the main character is
named, like \textit{Fahrenheit 451}, or \textit{The Hobbit}, that noun is the
name of the main character. In books where the main character is not named, a
different noun is used, like ``boy'' in \textit{The Alchemist}.

The results of our analysis are contained in the three
graphs~\ref{fig:fahrenheit_451_nouns},~\ref{fig:the_alchemist_nouns},~\ref{fig:the_hobbit_nouns},
which each contain the 30 most used nouns in a book.

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{fahrenheit_451_nouns}
	\caption{Most used nouns in Fahrenheit 451}\label{fig:fahrenheit_451_nouns}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{the_alchemist_nouns}
	\caption{Most used nouns in The Alchemist}\label{fig:the_alchemist_nouns}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{the_hobbit_nouns}
	\caption{Most used nouns in The Hobbit}\label{fig:the_hobbit_nouns}
\end{figure}

Our current plan is to use this information to focus summarization around these
important words. The results are not going to be pretty, but at least in some
books it might result in a summary that at least conveys the important plot
points.


\section{Evaluation}
Aside from simply reading the summarisation outputs of the algorithm, two
quantitative measures were proposed to evaluate them. It must be noted again
that the scores are expected to be quite low, because the nature of
summarisation leads to very different answers with no objective measure of how
``good'' they are. 

The nltk.translate.bleu\_score toolkit has a function, corpus\_blue, which can
be used for BLEU evaluation. BLEU was originally used for translations, but is
now also used to evaluate summarisations and is based on precision scores of
n-gram overlaps between documents and references. The corpus\_blue allows for
varying weights of 1- to 4-grams, and multiple golden references. 

In table~\ref{table:bleu_huckfinn}, example scores of running this BLEU
evaluation can be seen. In each case, one online summary was evaluated against
the other three as golden references. For pure 1-grams, the scores are always
higher than 2-grams, and indeed they were almost zero for 3- and 4-grams.
Another remark is that while scores over 0.6 seem quite high, the CliffsNotes
score is a low outlier at only 0.295 for the 1-gram case. Finally, it must be
noted that increasing the number of references also increases the score.
Because there are more versions of what a ``correct'' summary is, there is
probably also more overlap of the remaining summary to them. Because it is not
easy to obtain more summaries, the availability of only three to four per book
must simply be kept in mind as a limitation when it comes to evaluation.

\begin{table}[hbt]
	\centering
	\caption{Example BLEU 1-gram and 2-gram evaluation for online summaries of the Adventures of Huckleberry Finn.}\label{table:bleu_huckfinn}
	\begin{tabular}{l l l }
		\toprule
		\textbf{}   & \textbf{1-gram} & \textbf{2-gram} \\ \midrule
		CliffsNotes & 0.295           & 0.127           \\ \midrule
		SparkNotes  & 0.646           & 0.260           \\ \midrule
		GradeSaver  & 0.655           & 0.262           \\ \midrule
		Wikipedia   & 0.646           & 0.234           \\
		\bottomrule
	\end{tabular}
\end{table}

\section{Related Work}
Plenty of related work has been discussed earlier sections; \cite{Gaikwad2016} 
suggests classical approaches of current automatic summarization techniques, 
with plenty of abstractive and extractive methods. And it is said that 
approaches related to machine learning are only made with extractive methods. 
The new trend is shown, however, that many deep learning approaches adopt some 
abstractive methods: \cite{2016arXiv160206023N} and \cite{Rush2015}. Still, 
these papers are primarily targeting article-length summarization, so that our 
approaches would stand out in the targeting length of source text.

\section{Conclusions and Anticipated Problems}

