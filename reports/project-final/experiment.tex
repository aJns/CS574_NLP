\subsection{Dataset}
A dataset of ten novels and a play was collected from the internet, with at
least three summaries each. The books are mainly fictional classics, due to
their popularity on summary resources and often the expiration of their
copyright. Most of the summaries are sourced from CliffsNotes, SparkNotes and
GradeSaver. The books and summaries both vary quite vastly in length. All the
documents were saved as or converted to text files, to facilitate consistency
and ease of manipulation in Python.

\subsection{Evaluation}
Aside from simply reading the summarization outputs of the algorithm, two
quantitative measures are used to evaluate them.
The nltk.translate.bleu\_score toolkit has a function, corpus\_blue, which can
be used for BLEU evaluation. BLEU was originally used for translations, but is
now also used to evaluate summaries and is based on precision scores of
n-gram overlaps between documents and references. The corpus\_blue allows for
varying weights of 1- to 4-grams, and for multiple golden references. 

In table~\ref{table:bleu_huckfinn}, example scores of running this BLEU
evaluation can be seen. In each case, one online summary was evaluated against
the other three as golden references. For pure 1-grams, the scores are always
higher than 2-grams, and indeed they were almost zero for 3- and 4-grams.
Another remark is that while scores over 0.6 seem quite high, the CliffsNotes
score is a low outlier at only 0.295 for the 1-gram case. Finally, it must be
noted that increasing the number of references also increases the score.
Because there are more versions of what a ``correct'' summary is, there is
probably also more overlap of the remaining summary to them. Because it is not
easy to obtain more summaries, the availability of only three to four per book
must simply be kept in mind as a limitation when it comes to evaluation.

\begin{table}[H]
	\centering
	\caption{Example BLEU 1-gram and 2-gram evaluation for online summaries of the Adventures of Huckleberry Finn.}\label{table:bleu_huckfinn}
	\begin{tabular}{l l l }
		\toprule
		\textbf{}   & \textbf{1-gram} & \textbf{2-gram} \\ \midrule
		CliffsNotes & 0.295           & 0.127           \\ \midrule
		SparkNotes  & 0.646           & 0.260           \\ \midrule
		GradeSaver  & 0.655           & 0.262           \\ \midrule
		Wikipedia   & 0.646           & 0.234           \\
		\bottomrule
	\end{tabular}
\end{table}

Additionally, ROUGE (recall-oriented understudy for gisting evaluation) is also used for evaluation; the score is based on recall of n-gram overlaps between system and golden summaries. Because higher n-gram scores are so low, 1- and 2- grams of each BLEU and ROUGE were the main metrics used.

\subsection{Results}
Because of the novelty of the method, there were no established baselines to compare to. As ROUGE and BLEU scores would also vary wildly with the dataset used, it would also not make sense to compare these to other studies. In terms of purely quantitative scores, promising results were obtained, in the sense that the system summaries scored similarly to the golden summaries against each other.

In the experiments, the effect of changing various parameters was examined. The first of these was the sizes of the section that the book was broken into. This was changed between 10 and 100 sentences, while the number of sentences out from each chunk was kept constant at 

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{bleus_good}
	\caption{BLEU scores for varying the compression ratio per recursive cycle for the Alchemist. }\label{fig:bleus}
\end{figure}

