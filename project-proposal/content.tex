\section{Team details}
\begin{description}
  \item[Team:] Optimistic Cat Monkeys
  \item[Division of labor:] At the moment we are planning to split the work
    quite equally; everybody will take part in planning, implementation and
    documentation.
  \item[Diversity:] 3 different nationalities. One female member.
\end{description}


\section{Problem setup and motivation}

The problem we want to solve is summarising a long text, such as a novel or screenplay. We formally define this as summarising a monolingual (English) single-document text of over 50,000 words to one under 1000 words, while retaining the important essence of the information.\\
We think this is an interesting problem to solve, because summarising short articles is a fairly common application of NLP, but algorithms that can summarize long pieces are less present in literature. Even summarising multiple short documents, such as emails and news articles, is also an application receiving attention. Columbia Newsblaster is one popular news summary available online. \\
The applications of automatic book summaries specifically include it being less biased than people, and it can help decide whether to read/purchase a book through a quick overview. Additionally, it could help refresh one's memory of a book they read before, or help determine whether a certain reference book is relevant.\\
Thus, we have a place to start (short article summary), but still have to come up with new solutions. There exists a sizeable body of literature focusing on automatic text summarization, and a useful starting point is recent review papers. Upon gaining some more experience in NLP methods, we can then compare the various approaches in the review papers, both with machine learning and without, and choose what seems like a feasible first method. \cite{Kumar2016}


\section{Proposed approach}

Because of the complexity involved in abstractive methods, in our meetings we
have tried to think of extractive methods that would still work well on long
texts. One idea we have been focusing on is finding the differences in similar
books. Similar in this context means books that share the same genre and
``level'' of author, for example. The logic behind this idea is that books are
defined by their differences; If two books don't have any differences, they
obviously are the same book. Thus differences between books are very important
information, and thus suited for a summary. If we can extract enough
``difference data'' from books, we can use that data as keywords to summarize
texts. Or, at least we might gain more insights on what to focus on when
summarizing books.

Intuitively, big differences between books would be proper nouns. Thus it might
be simpler to just use a PoS tagger and center the summary around proper nouns.
However, this is not a very robust approach. Some books don't use a lot of
proper nouns, preferring to refer to their characters as pronouns (he, she), or
possibly as standard nouns (the man, the woman).  In any case, our approach
after getting the difference data is to apply quite a conventional summarizer
using that data. However, without using an abstractive method, we probably
can't expect results that come even close to a human.

Our approach is already different from existing ones, because we haven't been
able to find existing approaches to long text summarization.  Although it makes
our approach interesting, the lack of prior approaches has also been a problem.
We have had to invent our own approach with a distinct lack of knowledge about
the subject. Training data and evaluation methods also seem to be hard problems
to solve. We haven't found good looking data sets where we would have a wealth
of books with accompanying summaries. We are considering making our own
summaries for books we have read ourselves, but this is of course reliant on a
method that doesn't require a lot of data.


\section{Evaluation plan}
A summary could be judged on two main aspects: its retention of important content and the readability. However, quantitative measures are not ideal because there are many possible good answers, and "good" in this case is also subjective. Nevertheless, two methods are used in practice to evaluate summaries, and will be used in this project as well: ROUGE and BLEU metrics. ROUGE and BLEU are measures that compute variations of precision and recall of N-gram overlaps between the summary and the references \cite{nenkova2006summarization} \cite{lin2004rouge}. \\
Boths these metrics require a golden standard in the form of human-written summaries, and these will be taken from free online resources such as WikiSummaries, SparkNotes and CliffsNotes. A larger reference set is better, but realistically 2 summaries are expected to be found for each book on average, with the goal of at evaluating at least 10 book summaries. \\
BLEU and ROUGE cover content, but for small reference sets, would not be able to evaluate grammar and readability. A small number of the summaries could be read by the project members themselves and evaluated quantitatively for cohesion and readability, although this is not expected to be of a high quality based on knowledge of current NLP performance, because this is a challenge even in state-of-the-art systems.

\section{Plan of work}

\begin{enumerate}
  \item Get a simple summarising system that can summarize short texts working, by taking some currently used techniques
  \item Get another - or come up with another - system that can summarize long text to relatively shorter text with preserving the semantics.
  \item Connect those two systems (or multiple of systems in layer) to summarize long text to be much shorter.
  \item Adopt Evaluation plan used in semantic based approaches in abstractive methods to gauge accuracy of our system. (Up to here is planned to be done until next proposal)
  \item If it turned out to be no way to evaluate our system, then consider pivoting of the idea from the beginning
  \item If it seems producing some meaningful results, then stick to the initial plan
\end{enumerate}

